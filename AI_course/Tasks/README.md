# Прочитайте и опишите современную статью по Deep Learning

Область развивается очень быстро, поэтому постоянно приходится следить за последними достижениями - потренируемся это делать!

Практически все статьи в области Machine Learning бесплатно доступны в интернете, обычно на [arxiv.org](http://arxiv.org).

## Как найти статью, которую стоит прочесть?

Вариантов много!

- Дойти до ссылок на научные статьи из новостей про последние достижения AI, которые и так [прут](https://www.wired.com/story/comedian-machine-ai-learning-puns/) со [всех](https://venturebeat.com/2019/03/18/nvidia-researchers-debut-gaugan-ai-that-creates-fake-landscapes-that-look-real/) [щелей](https://www.technologyreview.com/s/613430/this-ai-generated-musak-shows-us-the-limit-of-artificial-creativity/).
- Прочитать в деталях одну из статей, упомянутых в лекциях курса (в слайдах всегда есть ссылки!)
- Посмотреть что является последним достижением (по-английски - State of the Art или SOTA) в какой-то конкретной задаче. Хороший ресурс, отслеживающий SOTA в большом классе задач - [https://paperswithсode.com](https://paperswithcode.com/) . Как пример можно посмотреть на:
  - [Semantic Segmentation](https://paperswithcode.com/task/semantic-segmentation)
  - [Question Answering](https://paperswithcode.com/task/question-answering)
- Поискать статьи по интересующим вас словам через  [http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)
- Следить за ссылками в сообществах связанных с Deep Learning

Ну и наконец несколько примеров статей:

- [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) - с чего начался перенос стиля а-ля Prisma
- [Super-convergence](https://arxiv.org/abs/1708.07120) - подход к очень быстрой тренировке сетей
- [SquuezeNet](https://arxiv.org/abs/1602.07360) - пример архитектуры, оптимизированной для мобильных устройств
- [Mark RCNN](https://arxiv.org/abs/1703.06870) - развитие RCNN для задачи Instance Segmentation
- [DenseNet](https://arxiv.org/abs/1608.06993) - развитие базовой архитектуры CNN после ResNet
- [Learning from Simulated and Unsupervised Images through Adversarial Training](https://arxiv.org/abs/1612.07828) - использование GANs для генерации примеров для тренировки
- [OpenAI GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - одна из последних архитектур для обучения языковых моделей
- [Transformer-XL](https://arxiv.org/abs/1901.02860) - развитие архитектуры Transformer
- [TacoTron2](https://arxiv.org/abs/1712.05884) - архитектура для синтеза речи

## Что значит описать?

Напишите пост с кратким описанием задачи, которая решается в статье, ключевой идеи и результата, и выложите его где-нибудь! Допускаются блог посты на любом сайте, посты в чатах telegram, ODS и ClosedCircles, видео на youtube итд итп.

Пожелания по посту:

- Описать задачу, решаюмую в статье, и главную метрику. Было бы здорово привести примеры сэмплов в датасете, сказать насколько он большой итд.
- Описать ключевую идею статьи "на пальцах", в идеале в контексте прошлых подходов или сравнения с неким бейзлайном. Обычно в статьях есть диаграмма, описывающая ключевые моменты, приведите ее и объясните что на ней что.
- Описать результаты. Часто в конце статьи есть таблица сравнения подхода из статьи с другими методами, ее тоже стоит привести и объяснить.
- Упомяните, что пост написан для http://dlcourse.ai !

Вот несколько постов такого типа:
- https://habr.com/ru/post/301084/
- https://habr.com/ru/company/ods/blog/352518/
- https://www.alexirpan.com/2017/02/22/wasserstein-gan.html

## Ок, и что потом?
Присылайте ссылки на свои посты в чаты для обсуждения курса!

Лучше из них мы запостим в слак ODS и устроим соревнование по количеству лайков!
